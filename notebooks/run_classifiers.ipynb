{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of classifiers on various datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "Global seed set to 1\n",
      "Global seed set to 1\n",
      "Global seed set to 1\n",
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "from scipy.io import arff\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import modules.dataset_configs as dc\n",
    "from modules.classifier_modules import UnivarTSClfDataModule\n",
    "from modules.classifier_modules import LSTMClf, LSTMClf_SaliencyGuidedTraining, AttentionLSTM\n",
    "from modules.classifier_modules import CNNClf, CNNClf_SaliencyGuidedTraining, BasicTCN\n",
    "from modules.helpers import seed\n",
    "\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\n",
    "    dc.config_Experiment1, dc.config_Experiment2, dc.config_Experiment3, dc.config_Experiment4, dc.config_Experiment5, \n",
    "    dc.config_Experiment6, dc.config_Experiment7, dc.config_Experiment8, dc.config_Experiment9,dc.config_Experiment10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    # network architecture\n",
    "    criterion = nn.BCELoss(),\n",
    "    hidden_size = 5,\n",
    "    num_layers = 1,\n",
    "    dropout = 0,\n",
    "    d_a = 50,\n",
    "    # training\n",
    "    batch_size = 10,\n",
    "    learning_rate = 0.001,\n",
    "    max_epochs = 1,                                                                                                      \n",
    "    num_workers = 8,    \n",
    "    # optimization\n",
    "    number_of_trials = 200                                                \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealWorldDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        data = df\n",
    "        self.length = df.shape[0]\n",
    "        labels = df.iloc[:, data.columns == \"label\"].values\n",
    "        sequence = data.iloc[:, data.columns != \"label\"].values\n",
    "        \n",
    "        sequence = sequence.reshape(*sequence.shape,1)\n",
    "        self.sequence = torch.tensor(sequence).float()\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sequence[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWRUDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        with open(\"../data/CWRU/CWRU_full_dataframe\", \"rb\") as input:\n",
    "            full_data = pickle.load(input)\n",
    "        \n",
    "        full_data = shuffle(full_data)\n",
    "        self.train_data = RealWorldDataset(full_data.iloc[:int(0.6*full_data.shape[0]),:])\n",
    "        self.val_data = RealWorldDataset(full_data.iloc[int(0.6*full_data.shape[0]):int(0.8*full_data.shape[0]),:])\n",
    "        self.test_data = RealWorldDataset(full_data.iloc[int(0.8*full_data.shape[0]):,:])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle = True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, shuffle = True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle = False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_CWRU = CWRUDataModule(batch_size = train_config['batch_size'])#, num_workers = train_config['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/CWRU/CWRU_full_dataframe\", \"rb\") as input:\n",
    "    full_data = pickle.load(input)\n",
    "    \n",
    "full_data = shuffle(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and optimization of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) LSTMClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    # Configurate model\n",
    "    config = experiment()\n",
    "    dm = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "     )\n",
    "    model = LSTMClf(\n",
    "        n_features = config['n_features'],\n",
    "        hidden_size = train_config['hidden_size'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        dropout = train_config['dropout'],\n",
    "        learning_rate = train_config['learning_rate']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        hidden_dim = trial.suggest_int(\"hidden_dim\", low=4, high=64, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",low=4, high=64)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=1,\n",
    "            logger = True,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, hidden_size = hidden_dim ,batch_size = batch_size)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=600)                 \n",
    "\n",
    "    model_name = LSTMClf.__name__ + \"_\" + experiment.__name__[7:]\n",
    "    print(model_name)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Validation loss: {}\".format(trial.value))\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    # Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = LSTMClf(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        hidden_size = study.best_params[\"hidden_dim\"],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        dropout = train_config[\"dropout\"],\n",
    "        learning_rate = study.best_params[\"learning_rate\"]\n",
    "    )\n",
    "    dm_final = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "    \n",
    "    #dm_final = CWRUDataModule(batch_size = study.best_params[\"batch_size\"])\n",
    "\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=1,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    deterministic=True,\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) LSTMClf_SaliencyGuidedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    # Configurate model\n",
    "    config = experiment()\n",
    "    dm = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "    )\n",
    "    model = LSTMClf_SaliencyGuidedTraining(\n",
    "        n_features = config['n_features'],\n",
    "        hidden_size = train_config['hidden_size'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        dropout = train_config[\"dropout\"],\n",
    "        learning_rate = train_config['learning_rate'],\n",
    "        mask_factor = config['mask_factor'],\n",
    "        kl_weight = config['kl_weight']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        hidden_dim = trial.suggest_int(\"hidden_dim\", low=4, high=64, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",low=4, high=64)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=1,\n",
    "            logger = True,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, hidden_size = hidden_dim ,batch_size = batch_size)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=600)                 \n",
    "\n",
    "    model_name = LSTMClf_SaliencyGuidedTraining.__name__ + \"_\" + experiment.__name__[7:]\n",
    "    print(model_name)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    # Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = LSTMClf_SaliencyGuidedTraining(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        hidden_size = study.best_params[\"hidden_dim\"],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        dropout = train_config[\"dropout\"],\n",
    "        learning_rate = study.best_params[\"learning_rate\"],\n",
    "        mask_factor = config['mask_factor'],\n",
    "        kl_weight = config['kl_weight']\n",
    "    )\n",
    "    dm_final = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=1,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    deterministic=True,\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) LSTM with input cell attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    # Configurate model\n",
    "    config = experiment()\n",
    "    dm = UnivarTSClfDataModule(\n",
    "         seq_len = config['seq_len'],\n",
    "         batch_size = train_config['batch_size'],\n",
    "         simulator = config['simulator'],\n",
    "         train_size = config['train_size'],\n",
    "         val_size = config['val_size'],\n",
    "         test_size = config['test_size'],\n",
    "     )\n",
    "    model = AttentionLSTM(\n",
    "        n_features = config['n_features'],\n",
    "        hidden_size = train_config['hidden_size'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        dropout = train_config[\"dropout\"],\n",
    "        learning_rate = train_config['learning_rate'],\n",
    "        d_a = train_config['d_a'],\n",
    "        r = config['attention_hops']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        hidden_dim = trial.suggest_int(\"hidden_dim\", low=4, high=64, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",low=4, high=64)\n",
    "        d_a = trial.suggest_int(\"d_a\",low=30, high=100)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=1,\n",
    "            logger = True,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, hidden_size = hidden_dim ,batch_size = batch_size, d_a = d_a)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"]*2, timeout=600)                 \n",
    "\n",
    "    model_name = AttentionLSTM.__name__ + \"_\" + experiment.__name__[7:]\n",
    "    print(model_name)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    # Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = AttentionLSTM(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        hidden_size = study.best_params[\"hidden_dim\"],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        dropout = train_config[\"dropout\"],\n",
    "        learning_rate = study.best_params[\"learning_rate\"],\n",
    "        d_a = study.best_params['d_a'],\n",
    "        r = config['attention_hops']\n",
    "    )\n",
    "    dm_final = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "  \n",
    "        \n",
    "    #dm_final = CWRUDataModule(batch_size = study.best_params[\"batch_size\"])        # need to be un-commented when using CWRU dataset\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=1,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    deterministic=True,\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) CNNClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    # Configurate model\n",
    "    config = experiment()\n",
    "    dm = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "    model = CNNClf(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        learning_rate = train_config['learning_rate']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",low=4, high=64)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=1,\n",
    "            logger = True,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=600)                 \n",
    "\n",
    "    model_name = CNNClf.__name__ + \"_\" + experiment.__name__[7:]\n",
    "    print(model_name)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Validation loss: {}\".format(trial.value))\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    # Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = CNNClf(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        learning_rate = study.best_params[\"learning_rate\"]\n",
    "    )\n",
    "    dm_final = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=1,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    deterministic=True,\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) CNN with Saliency guided training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    # Configurate model\n",
    "    config = experiment()\n",
    "    dm = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "    )\n",
    "    model = CNNClf_SaliencyGuidedTraining(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        learning_rate = train_config['learning_rate'],\n",
    "        mask_factor = config['mask_factor'],\n",
    "        kl_weight = config['kl_weight']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",low=4, high=64)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=1,\n",
    "            logger = True,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=600)                 \n",
    "\n",
    "    model_name = CNNClf_SaliencyGuidedTraining.__name__ + \"_\" + experiment.__name__[7:]\n",
    "    print(model_name)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    # Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = CNNClf_SaliencyGuidedTraining(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        num_layers = train_config['num_layers'],\n",
    "        learning_rate = study.best_params['learning_rate'],\n",
    "        mask_factor = config['mask_factor'],\n",
    "        kl_weight = config['kl_weight']\n",
    "    )\n",
    "    dm_final = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "\n",
    "    \n",
    "    #dm_final = CWRUDataModule(batch_size = study.best_params[\"batch_size\"])\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=1,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    deterministic=True,\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiment_list:\n",
    "\n",
    "    # Configurate model\n",
    "    config = experiment()\n",
    "    dm = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "    model = BasicTCN(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = train_config['batch_size'],\n",
    "        criterion = train_config['criterion'],\n",
    "        learning_rate = train_config['learning_rate']\n",
    "    )\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\",low=4, high=64)\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=train_config['max_epochs'],\n",
    "            gpus=1,\n",
    "            logger = True,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        hyperparameters = dict(learning_rate = learning_rate, batch_size = batch_size)\n",
    "        trainer.logger.log_hyperparams(hyperparameters)\n",
    "        trainer.fit(model, datamodule = dm)\n",
    "        return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner()\n",
    "    sampler = optuna.samplers.TPESampler(seed=1)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler = sampler)\n",
    "    study.optimize(objective, n_trials=train_config[\"number_of_trials\"], timeout=600)                 \n",
    "\n",
    "    model_name = BasicTCN.__name__ + \"_\" + experiment.__name__[7:]\n",
    "    print(model_name)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Validation loss: {}\".format(trial.value))\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    # Train model with optimized hyperparameters\n",
    "\n",
    "    final_model = BasicTCN(\n",
    "        n_features = config['n_features'],\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        criterion = train_config['criterion'],\n",
    "        learning_rate = study.best_params[\"learning_rate\"]\n",
    "    )\n",
    "    dm_final = UnivarTSClfDataModule(\n",
    "        seq_len = config['seq_len'],\n",
    "        batch_size = study.best_params[\"batch_size\"],\n",
    "        simulator = config['simulator'],\n",
    "        train_size = config['train_size'],\n",
    "        val_size = config['val_size'],\n",
    "        test_size = config['test_size'],\n",
    "        num_workers = train_config['num_workers']\n",
    "    )\n",
    "\n",
    "    \n",
    "    #dm_final = CWRUDataModule(batch_size = study.best_params[\"batch_size\"])\n",
    "\n",
    "    logger = TensorBoardLogger(save_location, name = model_name)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath= save_location + \"/\" + model_name,\n",
    "        filename= model_name + \"_checkpoints\"\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    final_trainer = pl.Trainer(\n",
    "    max_epochs=train_config['max_epochs'],\n",
    "    gpus=1,\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    deterministic=True,\n",
    "    )\n",
    "\n",
    "    final_trainer.fit(final_model, dm_final)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
